---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

tasks:
  check-kubectl:
    desc: "Validate that kubectl is installed"
    cmds:
      - |
        if ! command -v kubectl &> /dev/null; then
          echo -e "Error: 'kubectl' command not found"
          echo "Please install kubectl: https://kubernetes.io/docs/tasks/tools/"
          exit 1
        fi
    silent: true

  reconcile:
    deps: [check-kubectl]
    desc: "Apply Kubernetes configurations from yaml files and kustomization directories"
    summary: |
      Applies Kubernetes configuration defined in .yaml files and kustomization directories.

      It does the following:
      - Skips any directory named 'deprecated'
      - For directories with kustomization.yaml, runs 'kubectl apply -k .'
      - For files named ks.yaml, runs 'kubectl apply -f ks.yaml'
      - Skips .sops.yaml files and directories referencing them
    cmds:
      - |
        echo "Starting Kubernetes resource reconciliation..."

        # Function to check if kustomization references .sops.yaml
        check_sops_reference() {
          local kustomization_file="$1"
          if grep -q "\.sops\.yaml" "$kustomization_file" 2>/dev/null; then
            return 0  # Found sops reference
          fi
          return 1  # No sops reference
        }

        # Process all directories and files
        find . -type f -name "kustomization.yaml" -o -name "ks.yaml" | while read -r file; do
          # Skip deprecated paths
          if echo "$file" | grep -q "deprecated"; then
            echo "Skipping deprecated: $file"
            continue
          fi

          # Skip .sops.yaml files
          if echo "$file" | grep -q "\.sops\.yaml"; then
            echo "Skipping sops file: $file"
            continue
          fi

          if [[ "$file" == *"kustomization.yaml" ]]; then
            # Check for sops references
            if check_sops_reference "$file"; then
              echo "Skipping directory with .sops.yaml reference: $(dirname "$file")"
              continue
            fi

            dir=$(dirname "$file")
            echo -e "\033[32mApplying kustomization in: $dir\033[0m"
            kubectl apply -k "$dir" || {
              echo -e "\033[31mError applying kustomization in $dir\033[0m"
              exit 1
            }
          elif [[ "$file" == *"ks.yaml" ]]; then
            echo -e "\033[32mApplying kubectl file: $file\033[0m"
            kubectl apply -f "$file" || {
              echo -e "\033[31mError applying $file\033[0m"
              exit 1
            }
          fi
        done

        echo -e "\033[32mReconciliation complete!\033[0m"

  apply-secrets:
    deps: [check-kubectl]
    desc: Apply all Kubernetes secret files
    summary: |
      Finds and applies all files matching *secret.yaml pattern
    cmds:
      - |
        echo "Applying Kubernetes secrets..."
        find . -iname "*secret.yaml" -type f | while read -r secret_file; do
          echo "Applying: $secret_file"
          kubectl apply -f "$secret_file" || {
            echo -e "\033[31mError applying $secret_file\033[0m"
            exit 1
          }
        done
        echo -e "\033[32mAll secrets applied successfully!\033[0m"
    silent: true

  destroy-stuck-ns:
    deps: [check-kubectl]
    desc: Fix namespaces stuck in terminating state
    summary: |
      Removes finalizers from namespaces stuck in terminating state.
      This forces the deletion of stuck namespaces.
    prompt: "This will forcefully delete stuck namespaces. Continue?"
    cmds:
      - |
        echo "Finding namespaces stuck in terminating state..."

        # Get all terminating namespaces
        terminating_ns=$(kubectl get namespaces --field-selector=status.phase=Terminating -o json 2>/dev/null | jq -r '.items[].metadata.name' 2>/dev/null)

        if [ -z "$terminating_ns" ]; then
          echo "No namespaces stuck in terminating state found."
          exit 0
        fi

        echo "Found terminating namespaces:"
        echo "$terminating_ns"
        echo ""

        for ns in $terminating_ns; do
          echo "Removing finalizers from namespace: $ns"
          kubectl patch namespace "$ns" -p '{"metadata":{"finalizers":[]}}' --type=merge || {
            echo -e "\033[31mError patching namespace $ns\033[0m"
          }
        done

        echo -e "\033[32mStuck namespace cleanup complete!\033[0m"
    silent: true

  destroy-rancher:
    deps: [check-kubectl]
    desc: Completely remove Rancher from the cluster
    summary: |
      Tears down a Rancher deployment including all related resources,
      webhooks, and namespaces. Also runs cleanup scripts.
    prompt: This will completely remove Rancher from your cluster. Are you sure?
    cmds:
      - |
        if ! command -v helm &> /dev/null; then
          echo -e "\033[31mError: helm not found. Please install helm.\033[0m"
          exit 1
        fi

        echo -e "\033[33mStarting Rancher removal...\033[0m"

        # Delete webhook that breaks deployments when rancher fails to fully uninstall
        echo "Deleting Rancher webhook..."
        kubectl delete -n cattle-system MutatingWebhookConfiguration rancher.cattle.io 2>/dev/null || true

        # Uninstall Rancher via Helm
        echo "Uninstalling Rancher helm release..."
        helm uninstall rancher -n cattle-system 2>/dev/null || true

        # Run cleanup script if it exists
        if [ -f "hack/rancher_cleanup.py" ]; then
          echo "Installing Python kubernetes module..."
          python3 -m pip install kubernetes --quiet 2>/dev/null || pip3 install kubernetes --quiet

          echo "Running Rancher cleanup script..."
          cd hack && python3 rancher_cleanup.py && cd ..
        else
          echo -e "\033[33mWarning: hack/rancher_cleanup.py not found, skipping cleanup script\033[0m"
        fi

        # Delete various Rancher resources
        echo "Deleting Rancher resources..."
        kubectl delete apiservices v1beta1.metrics.k8s.io 2>/dev/null || true
        kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io rancher.cattle.io 2>/dev/null || true
        kubectl delete clusters.provisioning.cattle.io -n fleet-local local 2>/dev/null || true

        # Delete Rancher namespaces
        echo "Deleting Rancher namespaces..."
        for ns in cattle-system cattle-fleet-system cattle-fleet-local-system \
                  cattle-fleet-clusters-system cattle-global-nt \
                  cattle-impersonation-system cattle-global-data; do
          kubectl delete ns "$ns" 2>/dev/null || true
        done

        # Clean up stuck namespaces
        echo "Cleaning up any stuck namespaces..."
        task k8s:destroy-stuck-ns

        echo -e "\033[32mRancher removal complete!\033[0m"
    silent: true

  get-pods-all-ns:
    deps: [check-kubectl]
    desc: Get all pods across all namespaces
    cmds:
      - kubectl get pods --all-namespaces -o wide
    silent: true

  get-events:
    deps: [check-kubectl]
    desc: Get cluster events sorted by timestamp
    vars:
      NAMESPACE: '{{.NAMESPACE | default ""}}'
    cmds:
      - |
        if [ -n "{{.NAMESPACE}}" ]; then
          kubectl get events -n {{.NAMESPACE}} --sort-by='.lastTimestamp'
        else
          kubectl get events --all-namespaces --sort-by='.lastTimestamp'
        fi
    silent: true

  describe-node:
    deps: [check-kubectl]
    desc: Describe a specific node
    summary: |
      Get detailed information about a specific node

      Example:
        task k8s:describe-node NODE=k8s1
    vars:
      NODE: "{{.NODE}}"
    cmds:
      - |
        if [ -z "{{.NODE}}" ]; then
          echo -e "Error: NODE variable not set"
          echo "Usage: task k8s:describe-node NODE=nodename"
          exit 1
        fi
        kubectl describe node {{.NODE}}

  drain-node:
    deps: [check-kubectl]
    desc: Drain a node for maintenance
    summary: |
      Safely evict all pods from a node for maintenance

      Example:
        task k8s:drain-node NODE=k8s1
    vars:
      NODE: "{{.NODE}}"
    prompt: "This will drain node {{.NODE}}. Continue?"
    cmds:
      - |
        if [ -z "{{.NODE}}" ]; then
          echo -e "Error: NODE variable not set"
          echo "Usage: task k8s:drain-node NODE=nodename"
          exit 1
        fi
        kubectl drain {{.NODE}} --ignore-daemonsets --delete-emptydir-data

  uncordon-node:
    deps: [check-kubectl]
    desc: Uncordon a node after maintenance
    summary: |
      Mark a node as schedulable again after maintenance

      Example:
        task k8s:uncordon-node NODE=k8s1
    vars:
      NODE: "{{.NODE}}"
    cmds:
      - |
        if [ -z "{{.NODE}}" ]; then
          echo -e "Error: NODE variable not set"
          echo "Usage: task k8s:uncordon-node NODE=nodename"
          exit 1
        fi
        kubectl uncordon {{.NODE}}
