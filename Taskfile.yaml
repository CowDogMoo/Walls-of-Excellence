---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

set: [pipefail]

includes:
  ansible: https://raw.githubusercontent.com/CowDogMoo/taskfile-templates/main/ansible/Taskfile.yaml
  k8s: https://raw.githubusercontent.com/CowDogMoo/taskfile-templates/main/k8s/Taskfile.yaml
  pre-commit: https://raw.githubusercontent.com/CowDogMoo/taskfile-templates/main/pre-commit/Taskfile.yaml
  terraform: https://raw.githubusercontent.com/CowDogMoo/taskfile-templates/main/terraform/Taskfile.yaml
  renovate: https://raw.githubusercontent.com/CowDogMoo/taskfile-templates/main/renovate/Taskfile.yaml
  onepassword: https://raw.githubusercontent.com/CowDogMoo/taskfile-templates/main/secrets/onepassword/Taskfile.yaml
  bootstrap: ./.taskfiles/bootstrap/Taskfile.yaml
  test: ./.taskfiles/test/Taskfile.yaml
  guacamole: ./.taskfiles/guacamole/Taskfile.yaml
  proxmox: ./.taskfiles/proxmox/Taskfile.yaml

vars:
  INVENTORY: "k3s-ansible/inventory/cowdogmoo/hosts.ini"
  K3S_ANSIBLE_DIR: "k3s-ansible"
  K8S_NODES: "k8s1 k8s2 k8s3 k8s4 k8s5 k8s6 k8s7 k8s8"

tasks:
  default:
    desc: Default task that performs a comprehensive setup check for k3s deployment
    cmds:
      - task: ansible:check-ansible
      - task: check-inventory
      - |-
        echo "All checks passed successfully."
    silent: true

  check-inventory:
    desc: Check if inventory file exists
    cmds:
      - |
        if [ ! -f "{{.INVENTORY}}" ]; then
          echo "Inventory file not found: {{.INVENTORY}}"
          echo "Please ensure your k3s-ansible inventory is properly configured."
          exit 1
        fi
    silent: true

  run-cmd-all:
    desc: Run a command on all k8s nodes
    summary: |
      Run a command on all k8s nodes via SSH

      Examples:
        task run-cmd-all -- 'echo "hello"'
        task run-cmd-all -- 'ip addr | grep 192'
        task run-cmd-all -- 'sudo reboot'
    vars:
      CMD: "{{.CLI_ARGS}}"
    cmds:
      - |
        if [ -z "{{.CMD}}" ]; then
          echo "Error: no command provided"
          echo ""
          echo "Usage: task run-cmd-all -- 'your command here'"
          echo ""
          echo "Examples:"
          echo "  task run-cmd-all -- 'echo \"hello\"'"
          echo "  task run-cmd-all -- 'ip addr | grep 192'"
          echo "  task run-cmd-all -- 'uptime'"
          echo "  task run-cmd-all -- 'sudo reboot'"
          exit 1
        fi

        for node in {{.K8S_NODES}}; do
          echo "==================================="
          echo "Node: $node"
          echo "==================================="
          ssh $node "{{.CMD}}" || {
            echo "Error on $node: command failed"
            exit 1
          }
          echo ""
        done
        echo "Command completed on all nodes."
    silent: true

  run-cmd:
    desc: Run a command on a specific k8s node
    summary: |
      Run a command on a specific k8s node via SSH

      Examples:
        task run-cmd NODE=k8s1 -- 'echo "hello"'
        task run-cmd NODE=k8s2 -- 'ip addr | grep 192'
    vars:
      NODE: "{{.NODE}}"
      CMD: "{{.CLI_ARGS}}"
    cmds:
      - |
        if [ -z "{{.NODE}}" ]; then
          echo -e "Error: NODE variable not set"
          echo "Usage: task run-cmd NODE=k8s1 -- 'your command here'"
          exit 1
        fi

        if [ -z "{{.CMD}}" ]; then
          echo -e "Error: no command provided"
          echo "Usage: task run-cmd NODE=k8s1 -- 'your command here'"
          exit 1
        fi

        # Check if node is valid
        valid=false
        for node in {{.K8S_NODES}}; do
          if [ "$node" = "{{.NODE}}" ]; then
            valid=true
            break
          fi
        done

        if [ "$valid" = false ]; then
          echo -e "Error: {{.NODE}} is not a valid node"
          echo "Valid nodes: {{.K8S_NODES}}"
          exit 1
        fi

        echo -e "Running command on {{.NODE}}"
        ssh {{.NODE}} "{{.CMD}}"
    silent: true

  reboot:
    desc: Reboot a specific node or all nodes
    summary: |
      Reboot a specific k8s node or all nodes

      Examples:
        task reboot NODE=k8s1
        task reboot NODE=all
    vars:
      NODE: '{{.NODE | default ""}}'
    cmds:
      - |
        if [ -z "{{.NODE}}" ]; then
          echo -e "Error: NODE variable not set"
          echo "Usage: task reboot NODE=k8s1"
          echo "       task reboot NODE=all"
          exit 1
        fi

        if [ "{{.NODE}}" = "all" ]; then
          echo -e "Rebooting all nodes..."
          for node in {{.K8S_NODES}}; do
            echo -e "Rebooting $node"
            ssh $node "sudo reboot" || true
          done
          exit 0
        fi

        # Check if node is valid
        valid=false
        for node in {{.K8S_NODES}}; do
          if [ "$node" = "{{.NODE}}" ]; then
            valid=true
            break
          fi
        done

        if [ "$valid" = false ]; then
          echo -e "Error: {{.NODE}} is not a valid node"
          echo "Valid nodes: {{.K8S_NODES}}"
          exit 1
        fi

        echo -e "Rebooting {{.NODE}}"
        ssh {{.NODE}} "sudo reboot"
    silent: true

  reboot-all:
    desc: Reboot all k8s nodes
    prompt: This will reboot ALL k8s nodes. Are you sure?
    cmds:
      - task: reboot
        vars:
          NODE: all
    silent: true

  shutdown-cluster:
    desc: Shutdown the entire k8s cluster (gracefully or emergency)
    summary: |
      Shuts down the k8s cluster for power outages or planned maintenance.

      By default, performs graceful shutdown:
      1. Gracefully terminates all pods (except DaemonSets)
      2. Waits for pods to finish shutting down
      3. Shuts down worker nodes first
      4. Shuts down master nodes last

      This does NOT cordon nodes, so they will be ready when they boot back up.

      Set EMERGENCY=true for immediate shutdown (power outages):
        task shutdown-cluster EMERGENCY=true
    prompt: This will SHUTDOWN ALL k8s nodes. Are you sure?
    vars:
      EMERGENCY: '{{.EMERGENCY | default "false"}}'
      WORKER_NODES: "k8s2 k8s3 k8s4 k8s5 k8s6 k8s7 k8s8"
      MASTER_NODES: "k8s1"
    cmds:
      - |
        if [ "{{.EMERGENCY}}" = "false" ]; then
          echo "Starting graceful cluster shutdown..."
          echo ""

          # Gracefully terminate all non-DaemonSet pods
          echo "Step 1/4: Gracefully terminating all pods (except DaemonSets)..."
          kubectl get pods -A --field-selector=status.phase=Running -o json | \
            jq -r '.items[] | select(.metadata.ownerReferences[0].kind != "DaemonSet") | "\(.metadata.namespace) \(.metadata.name)"' | \
            while read ns pod; do
              kubectl delete pod -n "$ns" "$pod" --grace-period=30 --ignore-not-found=true 2>/dev/null || true
            done
          echo ""

          # Wait for pods to terminate
          echo "Step 2/4: Waiting up to 60 seconds for pods to terminate..."
          for i in {1..12}; do
            remaining=$(kubectl get pods -A --field-selector=status.phase=Running -o json | \
              jq '[.items[] | select(.metadata.ownerReferences[0].kind != "DaemonSet")] | length')
            if [ "$remaining" -eq 0 ]; then
              echo "  All pods terminated successfully."
              break
            fi
            echo "  $remaining pods still terminating... (${i}0s elapsed)"
            sleep 5
          done
          echo ""
        else
          echo "Starting EMERGENCY shutdown (skipping graceful termination)..."
          echo ""
        fi

        # Shutdown worker nodes
        echo "Step 3/4: Shutting down worker nodes..."
        for node in {{.WORKER_NODES}}; do
          echo "  Shutting down $node..."
          ssh $node "sudo shutdown -h now" 2>/dev/null || true
        done
        echo ""

        # Wait a bit before shutting down masters
        echo "Waiting 5 seconds..."
        sleep 5
        echo ""

        # Shutdown master nodes
        echo "Step 4/4: Shutting down master nodes..."
        for node in {{.MASTER_NODES}}; do
          echo "  Shutting down $node..."
          ssh $node "sudo shutdown -h now" 2>/dev/null || true
        done
        echo ""

        echo "Cluster shutdown initiated!"
        echo "All nodes should be powered off within 1-2 minutes."
        echo "Nodes will be ready for scheduling when they boot back up (no uncordoning needed)."
    silent: true

  ping:
    desc: Ping all k3s nodes using ansible
    deps: ["ansible:check-ansible", "check-inventory"]
    vars:
      HOSTS: '{{.HOSTS | default "all"}}'
      DEBUG: '{{.DEBUG | default "false"}}'
    cmds:
      - |
        echo "Pinging {{.HOSTS}} nodes..."
        {{if eq .DEBUG "false"}}ANSIBLE_PYTHON_INTERPRETER=auto_silent {{end}}\
        ansible {{.HOSTS}} -i "{{.INVENTORY}}" -m ping
    silent: true

  ping-masters:
    desc: Ping only master nodes
    cmds:
      - task: ping
        vars:
          HOSTS: master
    silent: true

  ping-nodes:
    desc: Ping only worker nodes
    cmds:
      - task: ping
        vars:
          HOSTS: node
    silent: true

  provision:
    desc: Provision k3s cluster (all nodes)
    deps: ["ansible:check-ansible", "check-inventory"]
    vars:
      GROUP: '{{.GROUP | default "all"}}'
      ANSIBLE_CFG: '{{.ANSIBLE_CFG | default "k3s-ansible/ansible.cfg"}}'
    cmds:
      - |
        echo "Provisioning k3s cluster on {{.GROUP}}..."

        # Always export ANSIBLE_CONFIG - either home config or custom
        export ANSIBLE_CONFIG="{{.ANSIBLE_CFG}}"
        echo "Using Ansible config: {{.ANSIBLE_CFG}}"

        ansible-playbook "{{.K3S_ANSIBLE_DIR}}/site.yml" \
          -i "{{.INVENTORY}}" \
          --limit {{.GROUP}}
    silent: true

  provision-masters:
    desc: Provision only master nodes
    cmds:
      - task: provision
        vars:
          GROUP: master
    silent: true

  provision-nodes:
    desc: Provision only worker nodes
    cmds:
      - task: provision
        vars:
          GROUP: node
    silent: true

  reset:
    desc: Reset k3s cluster (all nodes)
    deps: ["ansible:check-ansible", "check-inventory"]
    vars:
      GROUP: '{{.GROUP | default "all"}}'
    cmds:
      - |
        echo "Resetting k3s cluster on {{.GROUP}}..."
        ansible-playbook "{{.K3S_ANSIBLE_DIR}}/reset.yml" \
          -i "{{.INVENTORY}}" \
          --limit {{.GROUP}}
    silent: true

  reset-masters:
    desc: Reset only master nodes
    cmds:
      - task: reset
        vars:
          GROUP: master
    silent: true

  reset-nodes:
    desc: Reset only worker nodes
    cmds:
      - task: reset
        vars:
          GROUP: node
    silent: true

  # ============================================
  # Kubernetes Tasks
  # ============================================
  apply-secrets:
    deps: ["k8s:check-kubectl"]
    desc: Apply all Kubernetes secret files
    summary: |
      Finds and applies all files matching *secret.yaml pattern
    cmds:
      - |
        echo "Applying Kubernetes secrets..."
        find . -iname "*secret.yaml" -type f | while read -r secret_file; do
          echo "Applying: $secret_file"
          kubectl apply -f "$secret_file" || {
            echo -e "Error applying $secret_file"
            exit 1
          }
        done
        echo -e "All secrets applied successfully!"
    silent: true

  destroy-rancher:
    deps: ["k8s:check-kubectl", "k8s:helm:check-helm"]
    desc: Completely remove Rancher from the cluster
    summary: |
      Tears down a Rancher deployment including all related resources,
      webhooks, and namespaces. Also runs cleanup scripts.
    prompt: This will completely remove Rancher from your cluster. Are you sure?
    cmds:
      - |
        echo -e "Starting Rancher removal..."

        # Delete webhook that breaks deployments when rancher fails to fully uninstall
        echo "Deleting Rancher webhook..."
        kubectl delete -n cattle-system MutatingWebhookConfiguration rancher.cattle.io 2>/dev/null || true

        # Uninstall Rancher via Helm
        echo "Uninstalling Rancher helm release..."
        helm uninstall rancher -n cattle-system 2>/dev/null || true

        # Run cleanup script if it exists
        if [ -f "hack/rancher_cleanup.py" ]; then
          echo "Installing Python kubernetes module..."
          python3 -m pip install kubernetes --quiet 2>/dev/null || pip3 install kubernetes --quiet

          echo "Running Rancher cleanup script..."
          cd hack && python3 rancher_cleanup.py && cd ..
        else
          echo -e "Warning: hack/rancher_cleanup.py not found, skipping cleanup script"
        fi

        # Delete various Rancher resources
        echo "Deleting Rancher resources..."
        kubectl delete apiservices v1beta1.metrics.k8s.io 2>/dev/null || true
        kubectl delete mutatingwebhookconfigurations.admissionregistration.k8s.io rancher.cattle.io 2>/dev/null || true
        kubectl delete clusters.provisioning.cattle.io -n fleet-local local 2>/dev/null || true

        # Delete Rancher namespaces
        echo "Deleting Rancher namespaces..."
        for ns in cattle-system cattle-fleet-system cattle-fleet-local-system \
                  cattle-fleet-clusters-system cattle-global-nt \
                  cattle-impersonation-system cattle-global-data; do
          kubectl delete ns "$ns" 2>/dev/null || true
        done

        # Clean up stuck namespaces
        echo "Cleaning up any stuck namespaces..."

      - task: k8s:destroy-stuck-ns
      - |
        echo -e "Rancher removal complete!"
    silent: true

  reconcile:
    deps: ["k8s:check-kubectl"]
    desc: Apply Kubernetes configurations from yaml files and kustomization directories
    summary: |
      Applies Kubernetes configuration defined in .yaml files and kustomization directories.

      It does the following:
      - Skips any directory named 'deprecated'
      - For directories with kustomization.yaml, runs 'kubectl apply -k .'
      - For files named ks.yaml, runs 'kubectl apply -f ks.yaml'
      - Skips .sops.yaml files and directories referencing them
    cmds:
      - |
        echo "Starting Kubernetes resource reconciliation..."

        # Function to check if kustomization references .sops.yaml
        check_sops_reference() {
          local kustomization_file="$1"
          if grep -q "\.sops\.yaml" "$kustomization_file" 2>/dev/null; then
            return 0  # Found sops reference
          fi
          return 1  # No sops reference
        }

        # Process all directories and files
        find . -type f -name "kustomization.yaml" -o -name "ks.yaml" | while read -r file; do
          # Skip deprecated paths
          if echo "$file" | grep -q "deprecated"; then
            echo "Skipping deprecated: $file"
            continue
          fi

          # Skip .sops.yaml files
          if echo "$file" | grep -q "\.sops\.yaml"; then
            echo "Skipping sops file: $file"
            continue
          fi

          if [[ "$file" == *"kustomization.yaml" ]]; then
            # Check for sops references
            if check_sops_reference "$file"; then
              echo "Skipping directory with .sops.yaml reference: $(dirname "$file")"
              continue
            fi

            dir=$(dirname "$file")
            echo -e "Applying kustomization in: $dir"
            kubectl apply -k "$dir" || {
              echo -e "Error applying kustomization in $dir"
              exit 1
            }
          elif [[ "$file" == *"ks.yaml" ]]; then
            echo -e "Applying kubectl file: $file"
            kubectl apply -f "$file" || {
              echo -e "Error applying $file"
              exit 1
            }
          fi
        done

        echo -e "Reconciliation complete!"
